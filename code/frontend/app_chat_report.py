import sys 
sys.path.insert(0,"./code")

import streamlit as st
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
import os 
from dotenv import load_dotenv
from langchain_chroma import Chroma
from backend.model import embeeding_with_model



load_dotenv()

def chat_with_llm(q,docs):
    os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI-KEY")
    os.environ["OPENAI_ORGANIZATION"] = os.getenv("OPENAI_ORGANIZATION")
    llm = ChatOpenAI(openai_api_key=os.environ["OPENAI_API_KEY"], 
             openai_organization=os.environ["OPENAI_ORGANIZATION"],
             model=os.getenv("LLM_MODEL"),
             temperature=0,
             streaming=True)
    
    template = """
    # Answer rule
        - Let's think step by step.
        - List all references used.
        - You are an Answer bot, helping bank loan checker to understand the similarity text{knowledge} base on question{question}.
        - respond using Traditional Chinese (Taiwan), and do not use English.
        - You need to summarize and provide feedback on the important parts of your answer; don't give a long message.
    """

    prompt = PromptTemplate.from_template(template)
    llm_chain = prompt | llm
    feedback=llm_chain.invoke({'question':q,'knowledge':docs})
    return(feedback.content)





st.set_page_config(page_title="æˆä¿¡å ±å‘Šæ©Ÿå™¨äºº", page_icon="ğŸ“–", layout="wide")
st.header("ğŸ¤– æˆä¿¡å ±å‘Šæ©Ÿå™¨äºº")


with st.chat_message("assistant"):
    st.markdown("##### å“ˆå›‰ï¼ï¼ æˆ‘æ˜¯ä¸€ä½å”åŠ©ä½ å¿«é€Ÿé–±è¦½å¾µä¿¡å ±å‘Šçš„æ©Ÿå™¨äººï¼Œä¸¦ä¸”é‡å°ä½ çš„å•é¡Œå¿«é€Ÿå›ç­”~"  )

with st.chat_message("assistant"):
    with open('./loan_data/report/loan_report.txt', 'r', encoding='utf-8') as file :
        content = file.read()
        st.markdown(content)

with st.chat_message("assistant"):
    st.markdown("##### è«‹ä½ è©¢å•ä»»ä½•ç›¸é—œå•é¡Œï¼ï¼ï¼ ")


question = st.chat_input("è¼¸å…¥æ‚¨çš„å•é¡Œ")
if question:
    with st.chat_message("user"):
        st.markdown("##### "+question)
    with st.spinner("##### æ­£åœ¨ç‚ºæ‚¨æº–å‚™ç›¸é—œè¨Šæ¯ï¼Œè«‹ç¨ç­‰"):
        embedding_function=embeeding_with_model()
        db = Chroma(persist_directory=os.getenv("CHROMA_PATH"), embedding_function=embedding_function)
        docs=db.similarity_search(question,k=6)
        text=chat_with_llm(question,docs)
        with st.chat_message("assistant"):
            st.markdown(text)
        
            
      